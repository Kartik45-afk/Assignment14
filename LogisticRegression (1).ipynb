{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "##### #Ans.Logistic Regression is a statistical method used for binary classification problems where the dependent variable has only two possible outcomes such as Yes/No, Spam/Not Spam, or Pass/Fail. Despite its name, it is a classification algorithm and not a regression technique in the traditional sense.\n",
        "\n",
        "In Logistic Regression, instead of predicting continuous values like in Linear Regression, we predict the probability of an observation belonging to a particular category. This probability is calculated using the logistic or sigmoid function which converts any real-valued number into a value between 0 and 1. The sigmoid function is given as:\n",
        "\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "where z = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ\n",
        "\n",
        "The value of p represents the probability of the positive class. If p is greater than 0.5, we classify the observation as positive. If p is less than 0.5, we classify it as negative.\n",
        "\n",
        "Now let us understand how Logistic Regression differs from Linear Regression.\n",
        "\n",
        "1. Purpose and Output\n",
        "* Linear Regression is used for predicting continuous values such as prices or temperatures.\n",
        "* Logistic Regression is used for predicting probabilities and classifications such as spam or not spam.\n",
        "\n",
        "2. Mathematical Model\n",
        "* Linear Regression uses a straight line equation: y = β₀ + β₁x.\n",
        "* Logistic Regression uses the S-shaped sigmoid function: p = 1 / (1 + e^(-z)).\n",
        "\n",
        "3. Assumptions\n",
        "* Linear Regression assumes a linear relationship between variables and normally distributed errors.\n",
        "* Logistic Regression assumes a linear relationship between the log-odds of the dependent variable and the independent variables.\n",
        "\n",
        "4. Error Measurement\n",
        "* Linear Regression minimizes the sum of squared errors.\n",
        "* Logistic Regression uses maximum likelihood estimation and log-loss.\n",
        "\n",
        "5. Range of Predictions\n",
        "* Linear Regression can predict any real number from minus infinity to plus infinity.\n",
        "* Logistic Regression always predicts values between 0 and 1 as probabilities."
      ],
      "metadata": {
        "id": "1Uzue62v-AWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "##### #Ans.Role of the Sigmoid Function in Logistic Regression\n",
        "\n",
        "The sigmoid function plays a central role in Logistic Regression because it transforms the linear combination of input features into a probability value between 0 and 1.\n",
        "\n",
        "In Logistic Regression, the input is a linear equation of the form:\n",
        "\n",
        "z = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ\n",
        "\n",
        "The output of this linear equation can range from minus infinity to plus infinity, which is not suitable for classification since probabilities must always lie between 0 and 1.\n",
        "\n",
        "To solve this, the sigmoid (logistic) function is applied:\n",
        "\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "Roles of the sigmoid function:\n",
        "\n",
        "1. Probability Mapping - It compresses any real-valued number into the range [0,1], so the output can be interpreted as a probability.\n",
        "\n",
        "2. Decision Boundary - By setting a threshold (commonly 0.5), it helps in classifying outcomes into two categories. If p > 0.5, the observation is classified as positive. If p < 0.5, it is classified as negative.\n",
        "\n",
        "3. Smooth Gradient - The S-shaped curve of the sigmoid function provides smooth gradients, which are useful for optimization during training with methods like gradient descent."
      ],
      "metadata": {
        "id": "ftuJ8ac-Eash"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "##### #Ans.Regularization in Logistic Regression\n",
        "\n",
        "Regularization is a technique used in Logistic Regression (and other machine learning models) to prevent overfitting by adding a penalty term to the cost function. Overfitting occurs when the model learns not only the underlying pattern but also the noise in the training data, which reduces its performance on unseen data.\n",
        "\n",
        "In Logistic Regression, the cost function without regularization is based on maximum likelihood estimation. Regularization modifies this cost function by adding a penalty term that discourages the model from assigning too large weights (coefficients) to the features.\n",
        "\n",
        "There are mainly two types of regularization used:\n",
        "\n",
        "1. L1 Regularization (Lasso): Adds the absolute value of coefficients as a penalty term. It can shrink some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "2. L2 Regularization (Ridge): Adds the squared value of coefficients as a penalty term. It reduces the magnitude of coefficients but does not usually make them zero.\n",
        "\n",
        "Why is Regularization needed?\n",
        "\n",
        "* To prevent overfitting and improve generalization on new/unseen data.\n",
        "\n",
        "* To keep coefficients small, making the model more stable and less sensitive to fluctuations in training data.\n",
        "\n",
        "* In the case of L1, it also helps in simplifying the model by removing irrelevant features."
      ],
      "metadata": {
        "id": "wcVBoceeFd5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "##### #Ans.\n",
        "When building classification models like Logistic Regression, it is not enough to only look at accuracy. Different evaluation metrics are used to measure how well the model performs, especially when data is imbalanced. These metrics are important because they help us understand the strengths and weaknesses of a model and choose the best one for a given problem.\n",
        "\n",
        "Common Evaluation Metrics:\n",
        "\n",
        "1. Accuracy\n",
        "* Definition: The proportion of correctly classified observations out of the total observations.\n",
        "* Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "* Limitation: Can be misleading when classes are imbalanced.\n",
        "\n",
        "2. Precision\n",
        "* Definition: Out of all predicted positives, how many are actually positive.\n",
        "* Formula: Precision = TP / (TP + FP)\n",
        "* Importance: High precision means fewer false positives.\n",
        "\n",
        "3. Recall (Sensitivity or True Positive Rate)\n",
        "* Definition: Out of all actual positives, how many were correctly identified.\n",
        "* Formula: Recall = TP / (TP + FN)\n",
        "* Importance: High recall means fewer false negatives.\n",
        "\n",
        "4. F1 Score\n",
        "* Definition: Harmonic mean of precision and recall.\n",
        "* Formula: F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "* Importance: Useful when we want a balance between precision and recall.\n",
        "\n",
        "5. ROC Curve and AUC (Area Under Curve)\n",
        "* ROC Curve plots the True Positive Rate against the False Positive Rate at different thresholds.\n",
        "* AUC measures the overall ability of the model to distinguish between classes.\n",
        "* Importance: AUC closer to 1 indicates a better model.\n",
        "\n",
        "Why they are important:\n",
        "\n",
        "* They provide a deeper understanding of model performance beyond accuracy.\n",
        "* They help in selecting models suited for specific needs (e.g., fraud detection requires high recall, while spam detection may need high precision).\n",
        "* They allow comparison between different models on the same dataset."
      ],
      "metadata": {
        "id": "A9SJa0S2GBML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "# splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "# (Use Dataset from sklearn package)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression # Import LogisticRegression\n",
        "\n",
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIU8Yn5AHs9U",
        "outputId": "b1a3f6f1-4b92-43c6-fe3f-cde7ea70e329"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "# regularization (Ridge) and print the model coefficients and accuracy.\n",
        "# (Use Dataset from sklearn package)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(\n",
        "    penalty='l2',        # Ridge Regularization\n",
        "    C=1.0,               # Regularization strength (default = 1.0)\n",
        "    solver='liblinear',  # Suitable for smaller datasets\n",
        "    max_iter=1000        # Increase iterations to ensure convergence\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Model Coefficients with L2 Regularization:\\n\")\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Coefficient\": model.coef_[0]\n",
        "})\n",
        "print(coef_df.to_string(index=False))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTolRU2OIUMg",
        "outputId": "f1bf009f-8fcc-4742-ed7f-f750b3ab53eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients with L2 Regularization:\n",
            "\n",
            "                Feature  Coefficient\n",
            "            mean radius     2.132484\n",
            "           mean texture     0.152772\n",
            "         mean perimeter    -0.145091\n",
            "              mean area    -0.000829\n",
            "        mean smoothness    -0.142636\n",
            "       mean compactness    -0.415569\n",
            "         mean concavity    -0.651940\n",
            "    mean concave points    -0.344456\n",
            "          mean symmetry    -0.207613\n",
            " mean fractal dimension    -0.029774\n",
            "           radius error    -0.050034\n",
            "          texture error     1.442984\n",
            "        perimeter error    -0.303857\n",
            "             area error    -0.072569\n",
            "       smoothness error    -0.016159\n",
            "      compactness error    -0.001907\n",
            "        concavity error    -0.044886\n",
            "   concave points error    -0.037719\n",
            "         symmetry error    -0.041752\n",
            "fractal dimension error     0.005613\n",
            "           worst radius     1.232150\n",
            "          worst texture    -0.404581\n",
            "        worst perimeter    -0.036209\n",
            "             worst area    -0.027087\n",
            "       worst smoothness    -0.262631\n",
            "      worst compactness    -1.208985\n",
            "        worst concavity    -1.617969\n",
            "   worst concave points    -0.615251\n",
            "         worst symmetry    -0.742764\n",
            "worst fractal dimension    -0.116960\n",
            "\n",
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "# classification using multi_class='ovr' and print the classification report.\n",
        "# (Use Dataset from sklearn package)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into training and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression model for multiclass classification using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(\n",
        "    multi_class='ovr',  # One-vs-Rest strategy\n",
        "    solver='liblinear', # Suitable for smaller datasets\n",
        "    max_iter=1000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Multiclass Logistic Regression (OvR):\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm1nbrtBJW_G",
        "outputId": "5f122859-0e30-44f9-ae13-d2382b181743"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Multiclass Logistic Regression (OvR):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "# hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "# accuracy.\n",
        "# (Use Dataset from sklearn package)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\\n\")\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and cross-validation accuracy\n",
        "print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate best model on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_test = best_model.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"Test set accuracy with best parameters: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\\n\")\n",
        "\n",
        "# Classification report on test set\n",
        "print(\"Classification Report on Test Set with Best Model:\")\n",
        "print(classification_report(y_test, y_pred_test, target_names=cancer.target_names))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E61uWHZmKJm0",
        "outputId": "60885adf-e4a1-43d6-9eac-62ef0b04a080"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (569, 30)\n",
            "Classes: ['malignant' 'benign']\n",
            "Class distribution: [212 357]\n",
            "\n",
            "Best parameters found by GridSearchCV: {'C': 1, 'penalty': 'l2'}\n",
            "Best cross-validation accuracy: 0.9802\n",
            "Test set accuracy with best parameters: 0.9825 (98.25%)\n",
            "\n",
            "Classification Report on Test Set with Best Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.98      0.98        42\n",
            "      benign       0.99      0.99      0.99        72\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.98      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to standardize the features before training Logistic\n",
        "# Regression and compare the model's accuracy with and without scaling.\n",
        "# (Use Dataset from sklearn package)\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\\n\")\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\\n\")\n",
        "\n",
        "# Logistic Regression WITHOUT scaling\n",
        "print(\"Training Logistic Regression model WITHOUT scaling...\")\n",
        "model_no_scale = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scale:.4f} ({accuracy_no_scale*100:.2f}%)\\n\")\n",
        "\n",
        "# Logistic Regression WITH scaling\n",
        "print(\"Standardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Training Logistic Regression model WITH scaling...\")\n",
        "model_scaled = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy WITH scaling:    {accuracy_scaled:.4f} ({accuracy_scaled*100:.2f}%)\\n\")\n",
        "\n",
        "# Accuracy Comparison\n",
        "print(\"Comparison of Accuracy:\")\n",
        "print(f\"  Without Scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"  With Scaling:    {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O1ZSGTvLBi6",
        "outputId": "d5b62b43-cd55-4599-df30-023d3bdab340"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (569, 30)\n",
            "Classes: ['malignant' 'benign']\n",
            "Class distribution: [212 357]\n",
            "\n",
            "Training set shape: (455, 30)\n",
            "Test set shape: (114, 30)\n",
            "\n",
            "Training Logistic Regression model WITHOUT scaling...\n",
            "Accuracy WITHOUT scaling: 0.9561 (95.61%)\n",
            "\n",
            "Standardizing features...\n",
            "Training Logistic Regression model WITH scaling...\n",
            "Accuracy WITH scaling:    0.9825 (98.25%)\n",
            "\n",
            "Comparison of Accuracy:\n",
            "  Without Scaling: 0.9561\n",
            "  With Scaling:    0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you'd take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "##### #Ans.When predicting customer response to a marketing campaign with an imbalanced dataset (e.g., only 5% of customers respond), the following approach should be taken:\n",
        "\n",
        "1. Data Handling and Preprocessing\n",
        "* Feature Selection/Engineering: Identify meaningful features such as customer demographics, past purchases, browsing history, or engagement metrics.\n",
        "* Missing Values: Handle missing data via imputation or removal of rows/columns.\n",
        "* Categorical Variables: Encode categorical features using techniques like One-Hot Encoding or Label Encoding.\n",
        "\n",
        "2. Feature Scaling\n",
        "* Scale numeric features using StandardScaler or MinMaxScaler because Logistic Regression is sensitive to feature magnitudes, especially when using regularization.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "* Resampling Techniques:\n",
        "  * Oversampling: Increase the number of minority class samples using techniques like SMOTE.\n",
        "  * Undersampling: Reduce majority class samples to balance the dataset.\n",
        "* Class Weights: Use class_weight='balanced' in Logistic Regression to penalize misclassification of minority class more heavily.\n",
        "\n",
        "4. Model Building and Hyperparameter Tuning\n",
        "* Use Logistic Regression with regularization (L1 or L2) to prevent overfitting.\n",
        "* Tune hyperparameters like C (inverse of regularization strength) and penalty using GridSearchCV or RandomizedSearchCV.\n",
        "* For imbalanced data, consider scoring='roc_auc' during hyperparameter tuning rather than accuracy, because accuracy can be misleading.\n",
        "\n",
        "5. Model Evaluation\n",
        "* Avoid using accuracy alone; focus on metrics that are robust for imbalanced datasets:\n",
        "   * Precision: How many predicted responders are actually responders.\n",
        "   * Recall (Sensitivity): How many actual responders are correctly identified.\n",
        "   * F1-Score: Balance between precision and recall.\n",
        "   * ROC-AUC: Measures the model's ability to discriminate between responders and non-responders.\n",
        "* Use Confusion Matrix to understand true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "6. Deployment Considerations\n",
        "\n",
        "* Evaluate business impact: prioritize minimizing false negatives if missing a potential customer is costly.\n",
        "* Monitor model performance over time and retrain periodically as customer behavior changes.\n"
      ],
      "metadata": {
        "id": "Jlp9YMPhRDS9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOyYWfNwSZYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #"
      ],
      "metadata": {
        "id": "8Ru8fDV0FU2L"
      }
    }
  ]
}